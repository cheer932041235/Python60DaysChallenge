# Python60天打卡训练营
## 要求与目标
- 60天内完成50天打卡全额退款
- 面向0基础，从最简单的代码教学开始由浅入深，从0基础小白到熟悉深度学习代码
- 20多天即可完成机器学习项目的复现，60天即可完成复杂的深度学习项目复现

## 训练营进度
1. 第一期：4.20--6.18
2. 第二期：5.15--7.14
3. 目前一二期内容保持一致，在day60完结后会腾出手来修改和优化内容

## 目录
- day1：了解变量和格式化字符串输出
- day2：字符串的基础操作+比较运算
- day3：列表的基础操作+循环for语句
- day4：pandas库：数据的读取和连续变量缺失值的补全
- day5：pandas库：离散特征的独热编码
- day6：matplotlib与seaborn库：数据初步可视化
- day7：复习日
- day8：初识字典、标签编码和连续数据的处理
- day9：热力图和子图的绘制
- day10：模型的训练与评估（基础版本）
- day11: 超参数优化-----网格搜索和贝叶斯优化
- day12：启发式算法
- day13：不平衡数据集的处理策略
- day14：shap图的绘制
- day15：复习日
- day16：numpy数组的认识->shap值的深入理解
- day17：常见聚类算法实战
- day18: 聚类后簇类型的含义推断
- day19：常见特征筛选算法
- day20：奇异值分解原理讲解
- day21：常见的降维算法
- day22：复习日
- day23：pipeline管道
- day24：元组和os模块
- day25：异常处理
- day26：函数专题1：函数定义与参数
- day27：函数专题2：修饰器
- day28：类的定义和方法
- day29：复习日：类的装饰器
- day30：模块和库的导入
- day31：文件的规范拆分与写法
- day32：官方文档的阅读
- day33：MLP神经网络的训练
- day34：GPU训练及类的call方法
- day35：模型可视化与推理
- day36：复习日
- day37：早停策略和模型权重的保存
- day38：Dataset和Dateloader类
- day39：图像数据与显存
- day40：训练和测试的规范写法
- day41：简单CNN
- day42：Grad-CAM与HOOK函数
- day43：复习日
- day44：预训练模型
- day45：Tensorboard使用介绍
- day46：通道注意力（SE注意力）
- day47：注意力热图可视化
- day48：随机函数与广播机制
- day49: CBAM注意力
- day50：预训练模型+CBAM模块
- day51：复习日
- day52：神经网络调参指南
- day53：对抗生成网络
- day54：Inception网络及其思考
- day55：序列预测任务介绍
- day56：时序数据的检验
- day57：经典时序预测模型1
- day58：经典时序预测模型2
- day59：经典时序预测模型3
- day60：复习日


## 心得总结
至此，我们训练营的内容到这里就宣告结束了，知识密度基本覆盖了之前复试课的90%的知识点（NLP内容、RNN变体实在来不及说了），day60将会是自由学习（复习日），打卡训练营更新到现在，确实不容易，也没想到能完成工作量这么庞大的一次积累，也很感谢能够坚持到这里的各位，说一些打卡训练营的一些心得。

1. 在这个训练营中其实我的个人能力也是提升的，这是实话，因为过去在科研中，很多问题我知道如何做的，但是我并没有体系化的思考过这样做的原因，接着这次机会也让我把整个体系给串起来，个人觉得逻辑是完全闭环，该重点交代的都交代了，不至于出现知识点断层的情况。这也是为什么你们能看到很多人做的感觉很高大上，但是仔细去研究发现他甚至有一些逻辑上的错误，这是因为他没有扎实的基础，对很多问题在基础的认识上就有问题。

2. 学习是需要持续性的，之前大家肯定看到很多所谓的速成xxx，但是实际你思考下，哪怕是一个小的专题，比如时序预测的小demo，粗略估计没有十多天的内容你都没法理解整个闭环的流程，比如为什么和arima做对比？为什么用arima做特征工程？很多知识背后有很多隐知识在里面，只有实践和思考才能真正理解这些问题，这里只能堆时间，而且只有持续的堆时间才不至于下次重来------工科就是一个堆时间的科目，任何的短期捷径到后面肯定要付出双倍的时间。

## 后续安排
篇幅和设备限制，有一些任务必须搭配视频才能讲清楚，比如多个文件协同的任务等，会在科研班的后续提到，科研班的人工智能进阶部分也会沿用目前的这个体系化的形式更新---------代码+讲义（体系化梳理）+视频，这样每个大的知识点拆成多个小知识点的方式会极大提高输出的知识密度和逻辑闭环的可能性。

本期训练营之前的知识点还有一些由于篇幅问题无法交代，我会在科研班中交代，这里我列举出来一些和目前内容接轨的知识点，如果没有在科研班的同学建议大家自行学习：

●远程服务器的本地连接:
    1.服务器的路径地址
    2.数据的传输
    3.无卡模式等操作
●py文件训练时命令行的参数（文件中定义main（args），在通过parser.add_argument
●即可使用）
●tansfomer架构的代码解读
    1. 自注意力机制
    2. 多头注意力机制
    3. 交叉注意力机制
●张量维度变换的总结
●git工具的使用
●optuna超参数优化框架




